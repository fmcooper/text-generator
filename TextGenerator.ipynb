{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextGenerator.ipynb","version":"0.3.2","provenance":[{"file_id":"1tWqpTecJizthA8-D86Mf3O5KKLunjTGW","timestamp":1553364100392}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"gjJnkSnXte1i","colab_type":"text"},"cell_type":"markdown","source":["# Text generator\n","\n","<a href=\"https://colab.research.google.com/github/fmcooper/text-generator/blob/master/TextGenerator.ipynb\">\n","  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n","</a>\n","\n","---\n","Based on https://www.tensorflow.org/tutorials/sequences/text_generation"]},{"metadata":{"id":"kzF2PVq4vTxh","colab_type":"code","colab":{}},"cell_type":"code","source":["# !pip install tensorflow-gpu==2.0.0-alpha0\n","# !pip install --upgrade numpy\n","# !pip install --upgrade matplotlib"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8VXcYWIhte1j","colab_type":"code","outputId":"c0650e0e-e60f-4545-b234-8cf6ada7c3bb","executionInfo":{"status":"ok","timestamp":1553535022712,"user_tz":0,"elapsed":1490,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"cell_type":"code","source":["import sys\n","import os\n","import math\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","print(\"\\n---------- versions ----------\\n\")\n","print(\"python version: \" + sys.version)\n","print(\"numpy version: \" + np.__version__)\n","print(\"matplotlib version: \" + mpl.__version__)\n","print(\"tensorflow version: \" + tf.__version__)\n","print()\n","\n","tf.enable_eager_execution()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","NUM_TESTING = 10000\n","TESTING = False\n","EMBEDDING_DIMS = 256\n","RNN_UNITS = 1024\n","NUM_EPOCHS = 3\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","CHECKPOINT_DIR = F'/content/gdrive/My Drive/Colab/text-generator/checkpoints/' + \"epochs\" + str(NUM_EPOCHS) + \"_batchsize\" + str(BATCH_SIZE) + \"_embeddingdims\" + str(EMBEDDING_DIMS) + \"_rnnunits\" + str(RNN_UNITS) + \"/\"     # directory checkpoint weights of model are saved\n","ENTIRE_SAVE_PATH = CHECKPOINT_DIR + 'trained_entire_model.h5'   # file where entire model is saved"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","---------- versions ----------\n","\n","python version: 3.6.7 (default, Oct 22 2018, 11:32:17) \n","[GCC 8.2.0]\n","numpy version: 1.14.6\n","matplotlib version: 3.0.3\n","tensorflow version: 1.13.1\n","\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"Lju_9IIgte1p","colab_type":"text"},"cell_type":"markdown","source":["### Downloading the data"]},{"metadata":{"id":"9xNIXORqte1q","colab_type":"code","outputId":"0b49df46-e3dd-4aa9-8698-6ca7cc42833d","executionInfo":{"status":"ok","timestamp":1553535022715,"user_tz":0,"elapsed":1478,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["print(\"\\n---------- downloading data using keras ----------\\n\")\n","path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","---------- downloading data using keras ----------\n","\n"],"name":"stdout"}]},{"metadata":{"id":"jNmaO93fte1u","colab_type":"text"},"cell_type":"markdown","source":["### Exploring the data"]},{"metadata":{"id":"iMzXCrGCte1v","colab_type":"code","outputId":"27c882bc-4b97-4137-f573-6b7ecbf30616","executionInfo":{"status":"ok","timestamp":1553535022716,"user_tz":0,"elapsed":1464,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}},"colab":{"base_uri":"https://localhost:8080/","height":292}},"cell_type":"code","source":["print(\"\\n---------- exploring data ----------\\n\")\n","# decoding the data\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","if TESTING:\n","    text = text[0:NUM_TESTING]\n","# number of characters in the text\n","print('Total number characters: ' + str(len(text)))\n","print('First 100 characters: ')\n","print(text[:100])\n","vocab = sorted(set(text))\n","print('Number of unique characters in text: ' + str(len(vocab)))\n","print('Unique characters in text: ' + str(vocab))\n","vocab_size = len(vocab)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","---------- exploring data ----------\n","\n","Total number characters: 1115394\n","First 100 characters: \n","First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You\n","Number of unique characters in text: 65\n","Unique characters in text: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"],"name":"stdout"}]},{"metadata":{"id":"T20YQRJQte1y","colab_type":"text"},"cell_type":"markdown","source":["### Preparing data"]},{"metadata":{"id":"_FPW1DjSwsdK","colab_type":"text"},"cell_type":"markdown","source":["Creating maps between integers and text."]},{"metadata":{"id":"2aJ5EboHwwOG","colab_type":"code","outputId":"58db5a7b-fd60-4ea3-c556-0a3962c2c458","executionInfo":{"status":"ok","timestamp":1553535022717,"user_tz":0,"elapsed":1433,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"cell_type":"code","source":["print(\"\\n---------- preparing data ----------\\n\")\n","# mappings between indices and unique characters\n","indicesToChars = np.array(vocab)\n","charsToIndices = {}\n","for i, char in enumerate(vocab):\n","    charsToIndices[char] = i\n","print(\"character at index 20: \" + str(indicesToChars[20]))\n","print(\"character H has index: \" + str(charsToIndices['H']))\n","\n","# save the text as integers\n","textAsIntegers = np.array([charsToIndices[char] for char in text])\n","print(\"First 13 characters: \" + text[:13])\n","print(\"First 13 characters as integers: \" + str(textAsIntegers[:13]))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","---------- preparing data ----------\n","\n","character at index 20: H\n","character H has index: 20\n","First 13 characters: First Citizen\n","First 13 characters as integers: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"],"name":"stdout"}]},{"metadata":{"id":"YPWZVPEmxBRE","colab_type":"text"},"cell_type":"markdown","source":["Creating training examples and targets."]},{"metadata":{"id":"UIVfjawexAfx","colab_type":"code","outputId":"2e09f4df-e89e-4f2c-8f32-f8bafd425fc9","executionInfo":{"status":"ok","timestamp":1553535023749,"user_tz":0,"elapsed":2446,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}},"colab":{"base_uri":"https://localhost:8080/","height":598}},"cell_type":"code","source":["print(\"\\n---------- creating training examples and targets ----------\\n\")\n","# The input to the model will be a sequence of characters, and we train the model to predict \n","# the outputâ€”the following character at each time step.\n","\n","# We are going to break the text into chunks of length x (input) and then predict the next character (output). \n","# Our output will be the same length as the input, so as output we also include the last x characters \n","# of the input.\n","# E.g. for x=8, the input sequence is \"Harry Po\" and the output sequence is \"arry Pot\".\n","\n","# changing the data to a stream of characters\n","char_dataset = tf.data.Dataset.from_tensor_slices(textAsIntegers)\n","print(\"First 5 characters after slicing the data: \")\n","for i in char_dataset.take(5):\n","  print(indicesToChars[i.numpy()])\n","\n","# creating the input\n","def int_seq_to_string(item):\n","    seq = ''\n","    for char in item:\n","        seq = seq + indicesToChars[char.numpy()]\n","    return seq\n","\n","seq_length = 100\n","sequences = char_dataset.batch(seq_length+1, drop_remainder=True)   # length 101 for 100 input and 1 output\n","print(\"First 5 sequences of input: \")\n","for item in sequences.take(5):\n","    print(int_seq_to_string(item).encode(encoding='utf-8'))\n","\n","# creating the targets\n","def createTarget(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text\n","\n","# inputs and outputs\n","dataset = sequences.map(createTarget)\n","for input_example, target_example in  dataset.take(1):\n","    print(\"Example input and corresponding target: \")\n","    print ('Input data: ' + str(int_seq_to_string(input_example).encode(encoding='utf-8')))\n","    print ('Target data: ' + str(int_seq_to_string(target_example).encode(encoding='utf-8')))\n","\n","    # Each index of these vectors are processed as one time step. For the input at time step 0, \n","    # the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. \n","    # At the next timestep, it does the same thing but the RNN considers the previous step context in \n","    # addition to the current input character.\n","    for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n","        print(\"Step {:4d}\".format(i))\n","        print(\"  input: {} ({:s})\".format(input_idx, repr(indicesToChars[input_idx])))\n","        print(\"  expected output: {} ({:s})\".format(target_idx, repr(indicesToChars[target_idx])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","---------- creating training examples and targets ----------\n","\n","First 5 characters after slicing the data: \n","F\n","i\n","r\n","s\n","t\n","First 5 sequences of input: \n","b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n","b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n","b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n","b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n","Example input and corresponding target: \n","Input data: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target data: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n","Step    0\n","  input: 18 ('F')\n","  expected output: 47 ('i')\n","Step    1\n","  input: 47 ('i')\n","  expected output: 56 ('r')\n","Step    2\n","  input: 56 ('r')\n","  expected output: 57 ('s')\n","Step    3\n","  input: 57 ('s')\n","  expected output: 58 ('t')\n","Step    4\n","  input: 58 ('t')\n","  expected output: 1 (' ')\n"],"name":"stdout"}]},{"metadata":{"id":"twvH7gN5xPrf","colab_type":"text"},"cell_type":"markdown","source":["Creating training batches."]},{"metadata":{"id":"uIgJSefote1z","colab_type":"code","outputId":"f0a49f01-4e74-4149-f576-6d013f0b2172","executionInfo":{"status":"ok","timestamp":1553535023752,"user_tz":0,"elapsed":2430,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["# creating training batches\n","examples_per_epoch = len(text)\n","steps_per_epoch = examples_per_epoch//BATCH_SIZE\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences, \n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n","# it maintains a buffer in which it shuffles elements).\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(\"dataset: \" + str(dataset))\n","print(\"BATCH_SIZE: \" + str(BATCH_SIZE))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["dataset: <DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n","BATCH_SIZE: 64\n"],"name":"stdout"}]},{"metadata":{"id":"ZsqXUWzDte13","colab_type":"text"},"cell_type":"markdown","source":["### Building the model"]},{"metadata":{"id":"MFMxbe31te14","colab_type":"code","outputId":"80232399-691c-40bf-99e6-83276b034eca","executionInfo":{"status":"ok","timestamp":1553535025697,"user_tz":0,"elapsed":4359,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}},"colab":{"base_uri":"https://localhost:8080/","height":649}},"cell_type":"code","source":["print(\"\\n---------- building the model ----------\\n\")\n","if tf.test.is_gpu_available():\n","  rnn = tf.keras.layers.CuDNNGRU\n","else:\n","  import functools\n","  rnn = functools.partial(\n","    tf.keras.layers.GRU, recurrent_activation='sigmoid')\n","\n","# building the model\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n","    rnn(rnn_units, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True),\n","    tf.keras.layers.Dense(vocab_size)\n","  ])\n","  return model\n","\n","model = build_model(\n","  vocab_size = len(vocab), \n","  embedding_dim=EMBEDDING_DIMS, \n","  rnn_units=RNN_UNITS,\n","  batch_size=BATCH_SIZE)\n","\n","for input_example_batch, target_example_batch in dataset.take(1): \n","  example_batch_predictions = model(input_example_batch)\n","  print(\"example_batch_predictions.shape: \" + str(example_batch_predictions.shape) + \"# (batch_size, sequence_length, vocab_size)\")\n","\n","  model.summary()\n","\n","\n","print(\"\\n---------- test the model ----------\\n\")\n","# To get actual predictions from the model we need to sample from the output distribution, \n","# to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n","\n","# Note: It is important to sample from this distribution as taking the argmax of the distribution \n","# can easily get the model stuck in a loop.\n","\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1)\n","print(sampled_indices)\n","print(\"Input: \" + str(int_seq_to_string(input_example_batch[0]).encode(encoding='utf-8')))\n","print(\"Next Char Predictions: \" + str(int_seq_to_string(sampled_indices).encode(encoding='utf-8')))\n","\n","\n","print(\"\\n---------- optimiser and loss function ----------\\n\")\n","def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n","\n","\n","print(\"\\n---------- compile the model ----------\\n\")\n","model.compile(\n","    optimizer='adam',\n","    loss = loss)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","---------- building the model ----------\n","\n","example_batch_predictions.shape: (64, 100, 65)# (batch_size, sequence_length, vocab_size)\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (64, None, 256)           16640     \n","_________________________________________________________________\n","cu_dnngru_2 (CuDNNGRU)       (64, None, 1024)          3938304   \n","_________________________________________________________________\n","dense_2 (Dense)              (64, None, 65)            66625     \n","=================================================================\n","Total params: 4,021,569\n","Trainable params: 4,021,569\n","Non-trainable params: 0\n","_________________________________________________________________\n","\n","---------- test the model ----------\n","\n","tf.Tensor(\n","[ 8 51 33 27 14  7  9 20  6 34 39 22 47 55 54 35 44 29 23  0 52  3 17 30\n"," 58 46 18 59 34 36 15 23 53 51 41 29 25 11 29 27  6 20 56  1 14 51 40  0\n"," 40 43 42 42 17 46 26 28 17 38  9 49 39 25 35 53 36 57 59 59 42 25 34  6\n"," 16 43 46 56 15 27 36 22 17  1 35 14 23 28 25 12 30 15 19 21  8 60 32 56\n"," 13 43 31  9], shape=(100,), dtype=int64)\n","Input: b\"ease\\nyou, deliver.\\n\\nMENENIUS:\\nThere was a time when all the body's members\\nRebell'd against the bell\"\n","Next Char Predictions: b'.mUOB-3H,VaJiqpWfQK\\nn$ERthFuVXCKomcQM;QO,Hr Bmb\\nbeddEhNPEZ3kaMWoXsuudMV,DehrCOXJE WBKPM?RCGI.vTrAeS3'\n","\n","---------- optimiser and loss function ----------\n","\n","Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n","scalar_loss:       4.1748886\n","\n","---------- compile the model ----------\n","\n"],"name":"stdout"}]},{"metadata":{"id":"HhFZvL7rte17","colab_type":"text"},"cell_type":"markdown","source":["### Training the model"]},{"metadata":{"id":"2pyzZg84te1-","colab_type":"code","outputId":"6ba7a0e1-1ee1-45db-fe1c-7c6afda2c459","executionInfo":{"status":"ok","timestamp":1553545075819,"user_tz":0,"elapsed":22824,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}},"colab":{"base_uri":"https://localhost:8080/","height":632}},"cell_type":"code","source":["print(\"\\n---------- training the model ----------\\n\")\n","\n","# checkpoints\n","checkpoint_prefix = os.path.join(CHECKPOINT_DIR, \"ckpt_{epoch}\")\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n","\n","\n","history = model.fit(dataset.repeat(), epochs=NUM_EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])\n","\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["\n","---------- training the model ----------\n","\n","Epoch 1/3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["17427/17428 [============================>.] - ETA: 0s - loss: 0.8604WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f751ffc4710>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n","\n","Consider using a TensorFlow optimizer from `tf.train`.\n"],"name":"stdout"},{"output_type":"stream","text":["W0325 18:12:35.400642 140143457597312 network.py:1430] This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f751ffc4710>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n","\n","Consider using a TensorFlow optimizer from `tf.train`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n"],"name":"stdout"},{"output_type":"stream","text":["W0325 18:12:36.537815 140143457597312 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py:1436: update_checkpoint_state (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.train.CheckpointManager to manage checkpoints rather than manually editing the Checkpoint proto.\n"],"name":"stderr"},{"output_type":"stream","text":["17428/17428 [==============================] - 2531s 145ms/step - loss: 0.8604\n","Epoch 2/3\n","17427/17428 [============================>.] - ETA: 0s - loss: 1.5379WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f751ffc4710>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n","\n","Consider using a TensorFlow optimizer from `tf.train`.\n"],"name":"stdout"},{"output_type":"stream","text":["W0325 18:54:26.509651 140143457597312 network.py:1430] This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f751ffc4710>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n","\n","Consider using a TensorFlow optimizer from `tf.train`.\n"],"name":"stderr"},{"output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r17428/17428 [==============================] - 2510s 144ms/step - loss: 1.5379\n","Epoch 3/3\n","17427/17428 [============================>.] - ETA: 0s - loss: 1.2328WARNING:tensorflow:This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f751ffc4710>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n","\n","Consider using a TensorFlow optimizer from `tf.train`.\n"],"name":"stdout"},{"output_type":"stream","text":["W0325 19:36:15.066997 140143457597312 network.py:1430] This model was compiled with a Keras optimizer (<tensorflow.python.keras.optimizers.Adam object at 0x7f751ffc4710>) but is being saved in TensorFlow format with `save_weights`. The model's weights will be saved, but unlike with TensorFlow optimizers in the TensorFlow format the optimizer's state will not be saved.\n","\n","Consider using a TensorFlow optimizer from `tf.train`.\n"],"name":"stderr"},{"output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r17428/17428 [==============================] - 2509s 144ms/step - loss: 1.2328\n"],"name":"stdout"}]},{"metadata":{"id":"7PyUC_-HHfbD","colab_type":"text"},"cell_type":"markdown","source":["### Restoring from the last checkpoint"]},{"metadata":{"id":"BQm_aXnYte2F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"28afd59b-a88b-49a2-eb04-85805b5f5e90","executionInfo":{"status":"ok","timestamp":1553545075824,"user_tz":0,"elapsed":16,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}}},"cell_type":"code","source":["print(\"\\n---------- restoring the model ----------\\n\")\n","print(\"Last checkpoint: \" + str(tf.train.latest_checkpoint(CHECKPOINT_DIR)))\n","\n","# rebuild the model\n","model = build_model(vocab_size, EMBEDDING_DIMS, RNN_UNITS, batch_size=1)\n","model.load_weights(tf.train.latest_checkpoint(CHECKPOINT_DIR))\n","model.build(tf.TensorShape([1, None]))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["\n","---------- restoring the model ----------\n","\n","Last checkpoint: /content/gdrive/My Drive/Colab/text-generator/checkpoints/epochs3_batchsize64_embeddingdims256_rnnunits1024/ckpt_3\n"],"name":"stdout"}]},{"metadata":{"id":"ijVhXOzuI2RY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":683},"outputId":"d6eda2ad-1514-4a9a-bbe4-585cd0bd97db","executionInfo":{"status":"ok","timestamp":1553545075828,"user_tz":0,"elapsed":18,"user":{"displayName":"Frances Cooper","photoUrl":"","userId":"17037806151023713998"}}},"cell_type":"code","source":["print(\"\\n---------- generating text ----------\\n\")\n","def generate_text(model, start_string):\n","  # Number of characters to generate\n","  num_generate = 1000\n","\n","  # Converting our start string to numbers (vectorizing) \n","  input_eval = [charsToIndices[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  # Low temperatures results in more predictable text.\n","  # Higher temperatures results in more surprising text.\n","  # Experiment to find the best setting.\n","  temperature = 1.0\n","\n","  # Here batch size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # remove the batch dimension\n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # using a multinomial distribution to predict the word returned by the model\n","      predictions = predictions / temperature\n","      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n","      \n","      # We pass the predicted word as the next input to the model\n","      # along with the previous hidden state\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","      \n","      text_generated.append(indicesToChars[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))\n","\n","\n","print(generate_text(model, start_string=u\"ROMEO: \"))\n","\n","# The easiest thing you can do to improve the results it to train it for longer (try EPOCHS=30).\n","# You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions.\n","# more stuff on the "],"execution_count":30,"outputs":[{"output_type":"stream","text":["\n","---------- generating text ----------\n","\n","WARNING:tensorflow:From <ipython-input-30-5faa2ae923d2>:27: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.random.categorical instead.\n"],"name":"stdout"},{"output_type":"stream","text":["W0325 19:36:15.652512 140143457597312 deprecation.py:323] From <ipython-input-30-5faa2ae923d2>:27: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.random.categorical instead.\n"],"name":"stderr"},{"output_type":"stream","text":["ROMEO: Hoo! a cold contents to know,\n","Which to death blood wrong is:\n","Work you are rottends on him. But, thup\n","I cannot speak oof do, who is't\n","With with their virtue to this be disitur:\n","And, Greatful unkness; vell noisent to que, not long,\n","May hereag onto conget ds.\n","And in the regain to Firds will a listed,\n","Thinks he whose tonguel and mine heart:\n","And weary to Norfolks, his cast\n","For he what I stay undert ill heaven,\n","So never brideg-sir.\n","\n","PORLAGH:\n","Wear officer:\n","Madam, you shall have mercy: if, white,\n","And many ore, hast the exam'd:\n","How sweet man, before my cripless. Which boy of this night\n","We was are my place,\n","And respaish; And that I shows my dames,--afish five:\n","Now Lomy\n","That there was my side me so your loves:\n","Not not flesh and thing; whose blue of him to fold this wife and my husendershio's wife,\n","To an in coman, our sean to Frents?\n","\n","Whereof be a ragery too:\n","The heavens manner on thirgh, disconturness from many forth one the trwick's,\n","Now, Blook their assigg'd: the hours of the subfence\n","To many h\n"],"name":"stdout"}]}]}